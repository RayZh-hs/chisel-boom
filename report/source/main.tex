%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Wenneker Assignment
% LaTeX Template
% Version 2.0 (12/1/2019)
%
% This template originates from:
% http://www.LaTeXTemplates.com
%
% Authors:
% Vel (vel@LaTeXTemplates.com)
% Frits Wenneker
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{scrartcl} % Font size

\usepackage{xcolor}
\usepackage{hyperref}

\input{structure.tex} % Include the file specifying the document structure and custom commands

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{	
	\normalfont\normalsize
	\textsc{Shanghai Jiao Tong University}\\
	\vspace{25pt}
	\rule{\linewidth}{0.5pt}\\ % Thin top horizontal rule
	\vspace{20pt}
	{\huge Boom Lite CPU Report}\\ % The assignment title
	\vspace{12pt}
	\rule{\linewidth}{2pt}\\ % Thick bottom horizontal rule
	\vspace{12pt}
}

\author{Ruihang Zhang \quad Yuetian Wang}

\date{\normalsize\today}

\begin{document}

\maketitle

\section{Overview}

In the past month we have implemented, as the course project of Computer Architecture, a fully out-of-order RISC-V CPU named Boom Lite\footnote{The source code is available at \url{https://github.com/RayZh-hs/chisel-boom/}.}.

The CPU's architecture is inspired by the Berkeley Out-of-Order Machine (BOOM)\cite{Celio:EECS-2015-167}, though we made considerable simplifications and modifications to the original design to fit our time constraints. The final result is a CPU with more than 10 stages that accomplishes:

\begin{enumerate}
	\item Full support for the RV32I instruction set, including \texttt{JAL}, \texttt{JALR}, and bit-masked memory access.
	\item Full support for the M extension, using Wallace tree multipliers and restoring division algorithms.
	\item Unified memory system based on DRAM, along with ICache and DCache support.
	\item Multi-hierarchy branch predictor, combining a two-bit saturating BTB and a RAS.
	\item RAT-based OoO implementation to cut down on data movements.
	\item Partial OoO memory access as well as MMIO IO support.
\end{enumerate}

The CPU is written in Chisel, and has been synthesized to Verilog using Chisel's built-in FIRRTL compiler. The CPU achieved a clock frequency of over 517MHz on ASAP7 Demo, occupying under 15,000 um$^2$ of area.

\section{Architecture}

\subsection{Frontend Design}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=\textwidth]{figures/frontend.png}
	\caption{Boom Lite Frontend Diagram}
	\label{fig:frontend}
\end{figure*}

The frontend of Boom Lite consists of 5 stages, as is shown in Figure \ref{fig:frontend}. We will provide a brief overview of the stages below.

\subsubsection{Instruction Fetch}

The Fetch stage spans two cycles, the first of which fetches the instruction from ICache as well as consult the low-latency BTB for branch prediction. The second Fetch Stage gathers the fetched instruction and BTB feedback. If BTB predicts a taken branch, the PC is updated and the next-cycle fetch result will be killed.

\subsubsection{RAS \& Instruction Decode}

The instruction is then passed on to RAS (Return Address Stack) Adaptor and the Decoder simultaneously. The Decoder simply performs combinational logic to retrieve fields from the instruction. The RAS Adaptor picks out \texttt{JAL} and \texttt{JALR} instructions, and takes action on the RAS.
According to common ABI patterns, \texttt{CALL} and \texttt{RET} commands are speculated and the RAS modified thus. The component produces surprisingly sound prediction results, as was discovered in the profiling phase. If the predicted destination conflicts with BTB predictions, the former is disregarded and the frontend before the decoder flushed. The results from the two components are merged in the PDP (Pre-Dispatch Plexer), and the PC, Decoded Bundle, Predicted Target and RAS Stack Pointer are passed on to Rename \& Dispatch phases.

\subsubsection{Rename \& Dispatch}

The Rename phase consults the Free List for currently available physical registers and updates the Map Table (i.e. RAT, the Register Alias Table) accordingly, producing physical register indices for source and destination registers as well as documenting the stale pdst. The renamer also tracks the ROB ID for each instruction (which is deterministic since ROB is in-order enqueued).

The result is piped into a selective router known as the Dispatch Router, routing instructions to various Issue Buffers according to their types. We will cover the four issue buffers in the Backend Design section.

It is worth noting that the PC is \textbf{not} stored in the ROB. In fact, unless routed to the Branch Unit, the PC is discarded after the Dispatch phase. This design choice makes our ROB and normal issue buffers extremely small and efficient, having no need to pass around 32-bit information around.

\subsubsection{Notes on Branch Prediction}

As is mentioned above, the RAS stack pointer is passed down to the Dispatcher. This pointer is only passed to the Branch Unit, which, on misprediction recovery, will flag the old stack pointer to be restored. This will not eliminate the possibility of RAS being modified between the mispredicted branch and the recovery, but since it is speculative the approach is acceptable. We referenced various sources on the design and recovery of RAS\cite{desmet2005correct}\cite{skadron1998improving} before agreeing on this design.

Note that there are three cycles between Fetch Stage 1 and Decode, both sides included. This gives time for a full-fledged Global History Lookup Predictor to be implemented, and reusing the wiring of the plexer, but we had not yet implemented it. Had we done so the frontend would have resembled closely to a full TAGE predictor, which is the pattern adopted in BOOM.

\pagebreak  
\subsection{Backend Design}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=\textwidth]{figures/backend.png}
	\caption{Boom Lite Backend Diagram}
	\label{fig:backend}
\end{figure*}

The backend of Boom Lite follows a fairly standard out-of-order architecture, as is shown in Figure \ref{fig:backend}.

\subsubsection{The Re-Order Buffer}

In our Boom Lite design, the ROB is a circular buffer whose entries have minimal information.

% Code Block
\begin{verbatim}
class ROBEntry extends Bundle {
    val ldst = UInt(5.W)
    val pdst = UInt(PREG_WIDTH.W)
    val stalePdst = UInt(PREG_WIDTH.W)
    val isStore = Bool()
    val ready = Bool()
}
\end{verbatim}

Considering that there should be around 64 Physical Registers, the entire entry only takes up 19 bits, making it extremely area-efficient.

On misprediction, the ROB rolls back 2 entries at a time, comparing entries it contains with the mispredicted branch's ROB ID. Note that since everything is decoupled, all other units are still functional, alleviating the performance penalty.

\subsubsection{Issue Buffers \& Functional Pipelines}

There are four issue buffers in Boom Lite, each catering to different instruction types.

\begin{enumerate}
	\item \textbf{ALU (Arithmetic Logic Unit) Pipeline}: Handles RV32I arithmetic and logic instructions. Functional units have a 1-cycle latency.
	\item \textbf{MDU (Multiply Divide Unit) Pipeline}: Handles RV32M multiplication and division instructions. Multiplication is done using a Wallace tree multiplier, taking 3 cycles. Division is done using a restoring division algorithm where two bits are produced per cycle, taking 16 cycles in total.
	\item \textbf{BRU (Branch Unit)}: Handles all branch instructions. Similar to ALU, the functional unit has a 1-cycle latency. The unit signals misprediction on sight to the Misprediction Line, triggering flush and ROB rollback.
	\item \textbf{LSU (Load Store Unit) Pipeline}: This unit has been replaced later with LSQ (Load Store Queue) to support OoO memory access. It controls all memory access instructions.
\end{enumerate}

Before the execution of every instruction by the functional unit, all Issue Buffers share a common Data extraction trait which reads data from the Physical Register File.
Nothing is stored in the issue buffer itself but is directly passed on the the functional unit for processing.

\subsubsection{Broadcasting \& Commit}

All functional units broadcast results to the CDB, which is responsible for writing back to the Physical Register. It adopts a round robin arbitration scheme among functional units.

Commit is done asynchronously, the same in standard BOOM Design\footnote{An overview of standard BOOM architecture can be found here: \url{https://docs.boom-core.org/en/latest/sections/intro-overview/boom-pipeline.html}.}. When a store instruction is committed, in the in-order memory version of Boom Lite, the Committer alerts the front of the Memory Unit to perform the memory write. We will discuss the OoO memory design in the next subsection.

\subsubsection{Out-of-Order Memory Access}

\begin{figure*}[h!]
	\centering
	\includegraphics[width=\textwidth]{figures/ooo-memory.png}
	\caption{OoO Memory Access Diagram}
	\label{fig:lsq}
\end{figure*}

This is an experimental design refactor that has not yet been merged into the main branch. View it on the \texttt{feature/lsq} branch\footnote{\url{https://github.com/RayZh-hs/chisel-boom/tree/feature/lsq}.}.

Our Load Store Queue implements partial out-of-order execution and features store-to-load forwarding. It consists of a Load Queue and a Store Queue, connected via the Load Store Adaptor.

The Store Queue functions as a circular buffer following a Reservation Station style. Unlike other issue buffers, it captures and stores data upon broadcast. The Address Generation Unit (AGU) within the SQ calculates the effective address once the base is ready to facilitate store-to-load forwarding. A store is marked ready to commit when both address and data are available. It is first broadcast to the ROB for commit; once the ROB confirms the commit, the store is sent to the DCache.

The Load Queue is a non-ordered Reservation Station style buffer that issues loads speculatively. Upon enqueue, it records the current Store Queue tail to track program order.

Every cycle, the unit selects a ready load and checks for collisions against prior stores. Data bypassing is performed, in the process of which memory overlap issues are resolved.

\subsubsection{Memory Mapped I/O}

Our design fully employs MMIO to handle peripheral interactions. The stable release supports the \texttt{Exit} and \texttt{Put} devices, while a new \texttt{Get} device has been added in the \texttt{feature/mmio} branch\footnote{\url{https://github.com/RayZh-hs/chisel-boom/tree/feature/mmio}. This branch is spawned from \texttt{feature/lsq}.}.

Before making their way into DCache and therefore DRAM, memory accesses are checked against the MMIO address map, and those in the target range are sent to the MMIO Router. The router then dispatches the requests to the corresponding device modules, returning data to the CPU if necessary.

\pagebreak
\section{Performance}

We tested Boom Lite on a suite of benchmarks, among which are CPU Simulator Testcases from PPCA. An overview of the performance is shown below\footnote{Benchmarked on stable commit \texttt{d27697ced3bfbb90460d24d4375a2dbd9fc8dbca}. All optimizations except OoO memory access applied.}.

\begin{table}[h!]
    \centering
    % Load required package in preamble: \usepackage{multirow}
    \renewcommand{\arraystretch}{1.3} % Adds vertical breathing room
    \setlength{\tabcolsep}{12pt}      % Adds horizontal breathing room
    
    \begin{tabular}{|c|l|r|}
        \hline
        \textbf{Category} & \textbf{Metric} & \textbf{Value} \\
        \hline
        \hline
        % Block 1: 3 Rows
        \multirow{3}{*}{Branch Prediction} 
          & Total Branches & 1,015,143 \\
          & Total Mispredictions & 98,814 \\
          & \textbf{Misprediction Rate} & \textbf{9.73\%} \\
        \hline
        % Block 2: 3 Rows (Avg IPC is the last item here)
        \multirow{3}{*}{IPC \& Timing} 
          & Total Instructions & 5,466,223 \\
          & Total Cycles & 14,414,332 \\
          & \textbf{Average IPC} & \textbf{0.7010} \\
        \hline
        % Block 3: 6 Rows (Includes Rollback Time based on your image)
        \multirow{6}{*}{Queue Depths (Avg)} 
          & Fetch Queue & 1.15 \\
          & Issue ALU & 4.19 \\
          & Issue BRU & 2.48 \\
          & LSU Buffer & 5.23 \\
          & ROB & 21.94 \\
          & Avg. Rollback Time & 13.26 cycles \\
        \hline
        % Block 4: 4 Rows
        \multirow{4}{*}{Speculation} 
          & Total Dispatched & 7,850,082 \\
          & Squashed Instr. & 2,383,859 \\
          & Writeback Rate & 39.99\% \\
          & ROB Commit Rate & 37.92\% \\
        \hline
    \end{tabular}
    \caption{General Performance Statistics}
    \label{tab:perf-stats}
\end{table}

Note that the average IPC is calculated as the average number of IPC instead of the ratio of total instructions to total cycles. The few testcases with the most cycles tend to have abnormally low IPC, skewing the overall ratio, and dominating the instruction count. In fact, we observed that the average of per-cycle IPC is around 0.7010, while the overall ratio is only 0.3792, whereas only 2 testcases have IPC below 0.4.

Branch Prediction results outperform our expectations. The addition of the RAS significantly reduced mispredictions, resulting in a drop from 14.83\% to 9.73\%. We believe that by introducing a global/local history predictor, the rate can be further reduced to around 5\%.

We think the overall performance is acceptable for a single-issue OoO CPU, although the IPC can still be improved. In order to identify the bottleneck, we profiled the stages of the pipeline, and the results are shown below.

\begin{table}[h!]
    \centering
    % Increase padding for a cleaner look with vertical lines
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{8pt}

    \begin{tabular}{|l|l|r|r|}
        \hline
        \textbf{Pipeline Stage} & \textbf{Event / Stall Reason} & \textbf{Count / Cycles} & \textbf{Utilization} \\
        \hline
        \hline
        \textbf{Fetcher} & \textit{Total Active} & 14,415,034 & 100.00\% \\
        & Stall: Buffer & 4,215,922 & 29.25\% \\
        \hline
        \textbf{Decoder} & \textit{Total Active} & 12,233,822 & 84.87\% \\
        & Stall: Dispatch & 4,548,627 & 31.55\% \\
        \hline
        \textbf{Dispatcher} & \textit{Total Active} & 7,850,082 & 54.46\% \\
        & Stall: FreeList & 1,557,644 & 10.81\% \\
        & Stall: ROB & 1,095,108 & 7.60\% \\
        & Stall: Issue & 1,831,193 & 12.70\% \\
        \hline
        \textbf{Issue-ALU} & \textit{Total Active} & 1,782,905 & 12.37\% \\
        & Stall: Operands & 7,610,318 & 52.79\% \\
        \hline
        \textbf{Issue-BRU} & \textit{Total Active} & 1,016,658 & 7.05\% \\
        & Stall: Operands & 8,787,193 & 60.96\% \\
        \hline
        \textbf{Issue-Mult} & \textit{Total Active} & 606 & 0.00\% \\
        & Stall: Operands & 37,285 & 0.26\% \\
        \hline
        \textbf{LSU (Memory)} & \textit{Total Active} & 9,331,346 & 64.73\% \\
        & Stall: Commit & 666,588 & 4.62\% \\
        \hline
        \textbf{ROB Commit} & \textit{Total Active} & 5,466,223 & 37.92\% \\
        \hline
    \end{tabular}
    \caption{Pipeline stage utilization and stall analysis}
    \label{tab:stage-util}
\end{table}

After identifying that memory operations are the bottleneck for some testcases, we implemented the OoO memory access design described in the Architecture section. In addition, we applied several optimizations, which we will further discuss in the next major section. Several memory-intensive benchmarks, including the Matrix Multiplication testcase, saw significant IPC improvements.

\subsubsection{Matrix Multiplication}

\begin{table}[h!]
	\centering
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{10pt}
	\begin{tabular}{|c|l|r|}
		\hline
		\textbf{Category} & \textbf{Metric} & \textbf{Value} \\
		\hline
		\hline
		% Branch Prediction
		Branch Prediction & Misprediction Rate & 10.46\% \\
		\hline
		% IPC & Timing
		IPC \& Timing & {IPC} & {0.5396} \\
		\hline
		% Queue Depths
		\multirow{5}{*}{Queue Depths (Avg)} 
		  & Fetch Queue & 1.38 \\
		  & Issue ALU & 1.44 \\
		  & Issue BRU & 0.09 \\
		  & LSU Buffer & 5.43 \\
		  & ROB & 7.41 \\
		\hline
	\end{tabular}
	\caption{Matrix Multiplication: General Performance Statistics}
	\label{tab:matmul-perf}
\end{table}

Matrix Multiplication is a highly memory-intensive benchmark, involving tight-looped loads and stores. With the OoO memory access design and optimizations applied, we observed a significant IPC improvement from 0.3722 to 0.5396.

\subsubsection{Add to 100}

\begin{table}[h!]
	\centering
	\renewcommand{\arraystretch}{1.2}
	\setlength{\tabcolsep}{10pt}
	\begin{tabular}{|c|l|r|}
		\hline
		\textbf{Category} & \textbf{Metric} & \textbf{Value} \\
		\hline
		\hline
		% Branch Prediction
		Branch Prediction & Misprediction Rate & 2.73\% \\
		\hline
		% IPC & Timing
		IPC \& Timing & {IPC} & {0.5566} \\
		\hline
		% Queue Depths
		\multirow{5}{*}{Queue Depths (Avg)} 
		  & Fetch Queue & 1.20 \\
		  & Issue ALU & 0.44 \\
		  & Issue BRU & 0.17 \\
		  & LSU Buffer & 5.89 \\
		  & ROB & 4.28 \\
		\hline
	\end{tabular}
	\caption{Add to 100: General Performance Statistics}
	\label{tab:add100-perf}
\end{table}

Add to 100 is a simple benchmark that sums numbers from 1 to 100. It is not memory-intensive, and the overall performance exceeds that of Matrix Multiplication.

Note that the branch misprediction rate is significantly lower here since a simple BTB suffices for the large-constant loop structure.

\pagebreak
\section{Synthesis}

We utilized Silicon Compiler\footnote{\url{https://www.siliconcompiler.com/}}\cite{siliconcompiler} to synthesize our design from Verilog generated by Chisel. The synthesis was performed on the ASAP7 Demo, a 7nm process node. It yielded the following results.

\begin{figure*}[h!]
  \centering
  \includegraphics[width=0.4\textwidth]{figures/core-placement-density.png}
  % Gap in between images
  \hspace{0.05\textwidth}
  \includegraphics[width=0.4\textwidth]{figures/core-clocktree.png}
  \caption{Placement Density and Clock Tree on ASAP7 Demo (w. DRAM)}
  \label{fig:synthesis}
\end{figure*}

\begin{table}[h!]
    \centering
    \small
    \renewcommand{\arraystretch}{1.25}
    \setlength{\tabcolsep}{4pt}
    
    \begin{tabular}{|l|r|r|r|r|r|r|}
        \hline
        \textbf{Metric} & \textbf{Syn} & \textbf{Floorplan} & \textbf{Place} & \textbf{CTS} & \textbf{Route} & \textbf{Signoff} \\
        \hline
        \hline
        \multicolumn{7}{|l|}{\textit{Area \& Utilization}} \\
        \hline
        Cell Area ($\mu m^2$) & 5954.06 & 5637.14 & 6196.85 & 6353.44 & 6353.44 & 6353.44 \\
        Utilization (\%)      & ---     & 37.97\% & 41.74\% & 42.79\% & 42.79\% & 42.79\% \\
        \hline
        \multicolumn{7}{|l|}{\textit{Timing \& Frequency}} \\
        \hline
        Setup WNS (ns)        & ---     & -10.675 & -1.253  & -0.848  & -0.905  & -0.934 \\
        \textbf{Fmax (MHz)}   & ---     & ---     & 443.92  & 541.09  & 524.93  & \textbf{517.17} \\
        \hline
        \multicolumn{7}{|l|}{\textit{Power}} \\
        \hline
        Peak Power (mW)       & ---     & 29.77   & 32.25   & 35.38   & 36.08   & 36.19 \\
        Leakage (mW)          & ---     & 0.006   & 0.007   & 0.007   & 0.007   & 0.007 \\
        \hline
        \multicolumn{7}{|l|}{\textit{Design Complexity}} \\
        \hline
        Cells                 & 47,854  & 44,608  & 46,281  & 47,256  & 47,256  & 47,256 \\
        Registers             & 7,558   & 7,558   & 7,558   & 7,558   & 7,558   & 7,558 \\
        Nets                  & 48,288  & 43,236  & 44,909  & 45,769  & 45,769  & 45,769 \\
        \hline
        \multicolumn{7}{|l|}{\textit{Runtime}} \\
        \hline
        Task Time (s)         & 34.30   & 50.67   & 346.32  & 950.37  & 419.15  & 311.67 \\
        \hline
    \end{tabular}
    \caption{BoomCore Physical Design Flow Summary (ASAP7)}
    \label{tab:phys-design}
\end{table}

\pagebreak
\section{Optimization}

Throughout the development of Boom Lite, we have applied several optimizations to improve performance and efficiency. Some succeeded whereas others did not yield expected results. Below is a summary of some notable ones we have tried.

\subsection{Branch Prediction}

The original design consists of a naive BTB predictor, where the first jump of a hashed PC determines all the jumps thereafter. This approach has a high misprediction rate. In harder tests it can reach up to 25\%.

\begin{table}[h!]
    \centering
    \small % Compact font size
    \setlength{\tabcolsep}{5pt} % Reduced padding
    \renewcommand{\arraystretch}{1.15} % Slight vertical breathing room
    \label{tab:bp-comparison}
    \begin{tabular}{|l|r|r|r|}
        \hline
        \multirow{2}{*}{\textbf{Test Case}} & \multicolumn{3}{c|}{\textbf{Misprediction Rate}} \\
        \cline{2-4}
         & \textbf{Original} & \textbf{2-Bit Sat.} & \textbf{BTB + RAS} \\
        \hline
        \hline
        fibonacci   & 30.51\% & 34.45\% & 15.18\% \\
        matmul\_8x8 & 33.76\% & 29.70\% & 29.14\% \\
        superloop   & 11.14\% &  8.11\% &  7.97\% \\
        hanoi       & 23.75\% & 26.35\% & 18.23\% \\
        bulgarian   & 18.34\% & 16.58\% &  6.82\% \\
        queens      & 29.52\% & 24.92\% & 24.95\% \\
        \hline
        \textbf{AVERAGE} & \textbf{24.50\%} & \textbf{23.35\%} & \textbf{17.05\%} \\
        \hline
    \end{tabular}
    \caption{Branch Predictor Accuracy Comparison}
\end{table}

Introducing a 2-bit saturating counter for each BTB entry improved the accuracy only slightly, but the addition of RAS significantly reduced mispredictions, especially in recursive testcases like \texttt{fibonacci} and \texttt{hanoi}.

\subsection{Cache}

After migrating to unified DRAM, read and write has become more expensive when they cover consecutive bytes. The DRAM + Cache system does not improve performance in the simulation since all the memory accesses are 1-cycle only in the simulator, but in real-world scenarios the cache would greatly reduce memory access latency. In sight of this, we only verified the correctness of the cache system without further optimizations.

\subsection{Load Store Queues}

Before Out-of-Order Memory Access, the bottleneck of multiple testcases have been identified as the Load Store Unit (later the Memory Subsystem). Introducing OoO MA brought up the IPC considerably.

To make the most out of OoO MA, we attempted to create a bypassing unit between Load and Store Queues, which takes into account memory overlaps. This approach did increase the IPC further (but not as much as we had expected), but it had brought correctness issues into the system that we have yet to diagnose and resolve\footnote{Currently the \texttt{feature/lsq} branch is still under debugging and development, since it is known to fail on deep recursion testcases like \texttt{bulgarian} and \texttt{queens}.}.

A curious optimization is that we discovered that our initial assumption that the Broadcast Channel seldom fills up, thus seldom blocks issue buffers, is actually wrong in the context of memory access.
This happens when Store Memory Access commands awaiting Commit block the route to the CDB, causing operand-ready memory actions to pile up inside the issue queue. We resolved this issue by adding a small FIFO buffer before the CDB, which actually boosted IPC by a noticeable margin.

\subsection{Re-Order Buffer}

As is mentioned in the Architecture section, our ROB design is extremely compact, containing only essential information. That we keep PC information and Physical Register data minimal greatly reduces the area of our core.

An optimization we applied to the ROB is to decouple its rollback from the rest of the pipeline. On misprediction, the ROB rolls back independently while other units continue functioning; that is, all other units have a one-cycle delay in flushing only. This reduces the performance penalty of mispredictions.

In addition, we implemented a dual-flush mechanism in ROB, where two consecutive commands can be simultaneously flushed. This reduces the rollback time on mispredictions by nearly half, a considerable boost to performance.

We selected instruction-level flushing as opposed to snapshot-based flushing because we would have to stall if the snapshot buffer is full. By adopting our decoupled approach, we would not need to stall, though the rollback time is longer in our case.

\pagebreak
\bibliographystyle{plain}
\bibliography{references}

\end{document}
